{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T11:10:21.746471Z",
     "start_time": "2021-05-25T11:10:19.700306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Agent.ipynb\n",
      "importing Jupyter notebook from Trainer.ipynb\n",
      "importing Jupyter notebook from ReplayMemory.ipynb\n",
      "importing Jupyter notebook from Display.ipynb\n",
      "importing Jupyter notebook from Map.ipynb\n",
      "importing Jupyter notebook from BaseDisplay.ipynb\n",
      "importing Jupyter notebook from Grid.ipynb\n",
      "importing Jupyter notebook from DeviceManager.ipynb\n",
      "importing Jupyter notebook from IoTDevice.ipynb\n",
      "importing Jupyter notebook from Channel.ipynb\n",
      "importing Jupyter notebook from Shadowing.ipynb\n",
      "importing Jupyter notebook from State.ipynb\n",
      "importing Jupyter notebook from StateUtils.ipynb\n",
      "importing Jupyter notebook from BaseState.ipynb\n",
      "importing Jupyter notebook from BaseGrid.ipynb\n",
      "importing Jupyter notebook from ModelStats.ipynb\n",
      "importing Jupyter notebook from Physics.ipynb\n",
      "importing Jupyter notebook from GridActions.ipynb\n",
      "importing Jupyter notebook from GridPhysics.ipynb\n",
      "importing Jupyter notebook from Rewards.ipynb\n",
      "importing Jupyter notebook from GridRewards.ipynb\n",
      "importing Jupyter notebook from Environment.ipynb\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import distutils.util\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import import_ipynb\n",
    "from Agent import DDQNAgentParams, DDQNAgent\n",
    "from Trainer import DDQNTrainerParams, DDQNTrainer\n",
    "from Display import DHDisplay\n",
    "from Grid import GridParams, Grid\n",
    "from Physics import PhysicsParams, Physics\n",
    "from Rewards import RewardParams, Rewards\n",
    "from State import State\n",
    "from Environment import BaseEnvironment, BaseEnvironmentParams\n",
    "from GridActions import GridActions\n",
    "\n",
    "\n",
    "class EnvironmentParams(BaseEnvironmentParams):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grid_params = GridParams()\n",
    "        self.reward_params = RewardParams()\n",
    "        self.trainer_params = DDQNTrainerParams()\n",
    "        self.agent_params = DDQNAgentParams()\n",
    "        self.physics_params = PhysicsParams()\n",
    "\n",
    "\n",
    "class Environment(BaseEnvironment):\n",
    "    def __init__(self, params: EnvironmentParams):\n",
    "        self.display = DHDisplay()\n",
    "        super().__init__(params, self.display)\n",
    "\n",
    "        self.grid = Grid(params.grid_params, stats=self.stats)\n",
    "        self.rewards = Rewards(params.reward_params, stats=self.stats)\n",
    "        self.physics = Physics(params=params.physics_params, stats=self.stats)\n",
    "        self.agent = DDQNAgent(params.agent_params, self.grid.get_example_state(), \n",
    "                               self.physics.get_example_action(), stats=self.stats)\n",
    "        self.trainer = DDQNTrainer(params.trainer_params, agent=self.agent)\n",
    "\n",
    "        self.display.set_channel(self.physics.channel)\n",
    "\n",
    "        self.first_action = True\n",
    "        self.last_actions = []\n",
    "        self.last_rewards = []\n",
    "        self.last_states = []\n",
    "\n",
    "    def test_episode(self):\n",
    "        state = copy.deepcopy(self.init_episode())\n",
    "        self.stats.on_episode_begin(self.episode_count)\n",
    "        first_action = True\n",
    "        while not state.all_terminal:\n",
    "            for state.active_agent in range(state.num_agents):\n",
    "                if state.terminal:\n",
    "                    continue # continue 실행될 경우 아래 코드 실행되지 않고 건너뛴 뒤 다음 반복 시작\n",
    "                action = self.agent.get_exploitation_action_target(state)\n",
    "                if not first_action:\n",
    "                    reward = self.rewards.calculate_reward(self.last_states[state.active_agent],\n",
    "                                                           GridActions(self.last_actions[state.active_agent]), \n",
    "                                                           state)\n",
    "                    self.stats.add_experience(\n",
    "                        (self.last_states[state.active_agent], self.last_actions[state.active_agent], \n",
    "                         reward, copy.deepcopy(state)))\n",
    "\n",
    "                self.last_states[state.active_agent] = copy.deepcopy(state) #state 이동\n",
    "                self.last_actions[state.active_agent] = action #action 이동\n",
    "                state = self.physics.step(GridActions(action))\n",
    "                if state.terminal:\n",
    "                    reward = self.rewards.calculate_reward(self.last_states[state.active_agent],\n",
    "                                                           GridActions(self.last_actions[state.active_agent]), \n",
    "                                                           state)\n",
    "                    self.stats.add_experience(\n",
    "                        (self.last_states[state.active_agent], self.last_actions[state.active_agent], reward,\n",
    "                         copy.deepcopy(state)))\n",
    "\n",
    "            first_action = False\n",
    "\n",
    "        self.stats.on_episode_end(self.episode_count)\n",
    "        self.stats.log_testing_data(step=self.step_count)\n",
    "\n",
    "    def test_scenario(self, scenario):\n",
    "        state = copy.deepcopy(self.init_episode(scenario))\n",
    "        while not state.all_terminal:\n",
    "            for state.active_agent in range(state.num_agents):\n",
    "                if state.terminal:\n",
    "                    continue\n",
    "                action = self.agent.get_exploitation_action_target(state)\n",
    "                state = self.physics.step(GridActions(action))\n",
    "\n",
    "    def step(self, state: State, random=False):\n",
    "        for state.active_agent in range(state.num_agents):\n",
    "            if state.terminal:\n",
    "                continue\n",
    "            if random:\n",
    "                action = self.agent.get_random_action()\n",
    "            else:\n",
    "                action = self.agent.act(state)\n",
    "            if not self.first_action:\n",
    "                reward = self.rewards.calculate_reward(self.last_states[state.active_agent],\n",
    "                                                       GridActions(self.last_actions[state.active_agent]), state)\n",
    "                self.trainer.add_experience(self.last_states[state.active_agent], self.last_actions[state.active_agent],\n",
    "                                            reward, state)\n",
    "                self.stats.add_experience(\n",
    "                    (self.last_states[state.active_agent], self.last_actions[state.active_agent], reward,\n",
    "                     copy.deepcopy(state)))\n",
    "\n",
    "            self.last_states[state.active_agent] = copy.deepcopy(state)\n",
    "            self.last_actions[state.active_agent] = action\n",
    "            state = self.physics.step(GridActions(action))\n",
    "            if state.terminal:\n",
    "                reward = self.rewards.calculate_reward(self.last_states[state.active_agent],\n",
    "                                                       GridActions(self.last_actions[state.active_agent]), state)\n",
    "                self.trainer.add_experience(self.last_states[state.active_agent], self.last_actions[state.active_agent],\n",
    "                                            reward, state)\n",
    "                self.stats.add_experience(\n",
    "                    (self.last_states[state.active_agent], self.last_actions[state.active_agent], reward,\n",
    "                     copy.deepcopy(state)))\n",
    "\n",
    "        self.step_count += 1\n",
    "        self.first_action = False\n",
    "        return state\n",
    "\n",
    "    def init_episode(self, init_state=None):\n",
    "        state = super().init_episode(init_state)\n",
    "        #BaseEnvironment 클래스의 init_episode\n",
    "        self.last_states = [None] * state.num_agents\n",
    "        self.last_actions = [None] * state.num_agents\n",
    "        self.first_action = True\n",
    "        return state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
