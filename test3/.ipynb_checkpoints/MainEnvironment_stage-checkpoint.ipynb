{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd13d71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Agent_stage.ipynb\n",
      "importing Jupyter notebook from Trainer_stage.ipynb\n",
      "importing Jupyter notebook from ReplayMemory_stage.ipynb\n",
      "importing Jupyter notebook from Display_stage.ipynb\n",
      "importing Jupyter notebook from Map_stage.ipynb\n",
      "importing Jupyter notebook from BaseDisplay_stage.ipynb\n",
      "importing Jupyter notebook from Grid_stage.ipynb\n",
      "importing Jupyter notebook from DeviceManager_stage.ipynb\n",
      "importing Jupyter notebook from IoTDevice_stage.ipynb\n",
      "importing Jupyter notebook from Channel_stage.ipynb\n",
      "importing Jupyter notebook from Shadowing_stage.ipynb\n",
      "importing Jupyter notebook from State_stage.ipynb\n",
      "importing Jupyter notebook from StateUtils_stage.ipynb\n",
      "importing Jupyter notebook from BaseState_stage.ipynb\n",
      "importing Jupyter notebook from BaseGrid_stage.ipynb\n",
      "importing Jupyter notebook from ModelStats_stage.ipynb\n",
      "importing Jupyter notebook from Physics_stage.ipynb\n",
      "importing Jupyter notebook from GridActions_stage.ipynb\n",
      "importing Jupyter notebook from GridPhysics_stage.ipynb\n",
      "importing Jupyter notebook from Rewards_stage.ipynb\n",
      "importing Jupyter notebook from GridRewards_stage.ipynb\n",
      "importing Jupyter notebook from Environment_stage.ipynb\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import distutils.util\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import import_ipynb\n",
    "from Agent_stage import DDQNAgentParams, DDQNAgent\n",
    "from Trainer_stage import DDQNTrainerParams, DDQNTrainer\n",
    "from Display_stage import DHDisplay\n",
    "from Grid_stage import GridParams, Grid\n",
    "from Physics_stage import PhysicsParams, Physics\n",
    "from Rewards_stage import RewardParams, Rewards\n",
    "from State_stage import State\n",
    "from Environment_stage import BaseEnvironment, BaseEnvironmentParams\n",
    "from GridActions_stage import GridActions\n",
    "\n",
    "\n",
    "class EnvironmentParams(BaseEnvironmentParams):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grid_params = GridParams()\n",
    "        self.reward_params = RewardParams()\n",
    "        self.trainer_params = DDQNTrainerParams()\n",
    "        self.agent_params = DDQNAgentParams()\n",
    "        self.physics_params = PhysicsParams()\n",
    "\n",
    "\n",
    "class Environment(BaseEnvironment):\n",
    "    def __init__(self, params: EnvironmentParams):\n",
    "        self.display = DHDisplay()\n",
    "        super().__init__(params, self.display)\n",
    "\n",
    "        self.grid = Grid(params.grid_params, stats=self.stats)\n",
    "        self.rewards = Rewards(params.reward_params, stats=self.stats)\n",
    "        self.physics = Physics(params=params.physics_params, stats=self.stats)\n",
    "        self.agent = DDQNAgent(params.agent_params, self.grid.get_example_state(), \n",
    "                               self.physics.get_example_action(), stats=self.stats)\n",
    "        self.trainer = DDQNTrainer(params.trainer_params, agent=self.agent)\n",
    "\n",
    "        self.display.set_channel(self.physics.channel)\n",
    "\n",
    "        self.first_action = True\n",
    "        self.last_actions = []\n",
    "        self.last_rewards = []\n",
    "        self.last_states = []\n",
    "\n",
    "    def test_episode(self):\n",
    "        state = copy.deepcopy(self.init_episode())\n",
    "        self.stats.on_episode_begin(self.episode_count)\n",
    "        first_action = True\n",
    "        while not state.all_terminal:\n",
    "            for state.active_agent in range(state.num_agents):\n",
    "                if state.terminal:\n",
    "                    continue # continue 실행될 경우 아래 코드 실행되지 않고 건너뛴 뒤 다음 반복 시작\n",
    "                action = self.agent.get_exploitation_action_target(state)\n",
    "                if not first_action:\n",
    "                    reward = self.rewards.calculate_reward(self.last_states[state.active_agent],\n",
    "                                                           GridActions(self.last_actions[state.active_agent]), \n",
    "                                                           state)\n",
    "                    self.stats.add_experience(\n",
    "                        (self.last_states[state.active_agent], self.last_actions[state.active_agent], \n",
    "                         reward, copy.deepcopy(state)))\n",
    "\n",
    "                self.last_states[state.active_agent] = copy.deepcopy(state) #state 이동\n",
    "                self.last_actions[state.active_agent] = action #action 이동\n",
    "                state = self.physics.step(GridActions(action))\n",
    "                \n",
    "                if state.terminal:\n",
    "                    reward = self.rewards.calculate_reward(self.last_states[state.active_agent],\n",
    "                                                           GridActions(self.last_actions[state.active_agent]), \n",
    "                                                           state)\n",
    "                    self.stats.add_experience(\n",
    "                        (self.last_states[state.active_agent], self.last_actions[state.active_agent], reward,\n",
    "                         copy.deepcopy(state)))\n",
    "\n",
    "            first_action = False\n",
    "\n",
    "        self.stats.on_episode_end(self.episode_count)\n",
    "        self.stats.log_testing_data(step=self.step_count)\n",
    "\n",
    "    def test_scenario(self, scenario):\n",
    "        state = copy.deepcopy(self.init_episode(scenario))\n",
    "        while not state.all_terminal:\n",
    "            for state.active_agent in range(state.num_agents):\n",
    "                if state.terminal:\n",
    "                    continue\n",
    "                action = self.agent.get_exploitation_action_target(state)\n",
    "                state = self.physics.step(GridActions(action))\n",
    "\n",
    "\n",
    "    def step(self, state: State, random=False):\n",
    "        for state.active_agent in range(state.num_agents):\n",
    "            if state.terminal:\n",
    "                continue\n",
    "            if random:\n",
    "                action = self.agent.get_random_action()\n",
    "            else:\n",
    "                action = self.agent.act(state)\n",
    "            if not self.first_action:\n",
    "                reward = self.rewards.calculate_reward(self.last_states[state.active_agent],\n",
    "                                                       GridActions(self.last_actions[state.active_agent]), state)\n",
    "                self.trainer.add_experience(self.last_states[state.active_agent], self.last_actions[state.active_agent],\n",
    "                                            reward, state)\n",
    "                self.stats.add_experience(\n",
    "                    (self.last_states[state.active_agent], self.last_actions[state.active_agent], reward,\n",
    "                     copy.deepcopy(state)))\n",
    "\n",
    "            self.last_states[state.active_agent] = copy.deepcopy(state)\n",
    "            self.last_actions[state.active_agent] = action\n",
    "            state = self.physics.step(GridActions(action))\n",
    "\n",
    "            if state.terminal:\n",
    "                reward = self.rewards.calculate_reward(self.last_states[state.active_agent],\n",
    "                                                       GridActions(self.last_actions[state.active_agent]), state)\n",
    "                self.trainer.add_experience(self.last_states[state.active_agent], self.last_actions[state.active_agent],\n",
    "                                            reward, state)\n",
    "                self.stats.add_experience(\n",
    "                    (self.last_states[state.active_agent], self.last_actions[state.active_agent], reward,\n",
    "                     copy.deepcopy(state)))\n",
    "\n",
    "        self.step_count += 1\n",
    "        self.first_action = False\n",
    "        return state\n",
    "\n",
    "    def init_episode(self, init_state=None):\n",
    "        state = super().init_episode(init_state)\n",
    "        #BaseEnvironment 클래스의 init_episode\n",
    "        self.last_states = [None] * state.num_agents\n",
    "        self.last_actions = [None] * state.num_agents\n",
    "        self.first_action = True\n",
    "        return state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
