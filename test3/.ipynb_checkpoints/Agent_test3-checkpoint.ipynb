{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Concatenate, Input, AvgPool2D\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def print_node(x): #예 : print_node(3) -> 3 / 3 로 3이 2번 나옴\n",
    "    print(x)       #예 : print_node('3') -> 3 / '3' \n",
    "    return x\n",
    "\n",
    "\n",
    "class DDQNAgentParams:\n",
    "    def __init__(self):\n",
    "        # Convolutional part config\n",
    "        self.conv_layers = 2\n",
    "        self.conv_kernel_size = 5\n",
    "        self.conv_kernels = 16\n",
    "\n",
    "        # Fully Connected config\n",
    "        self.hidden_layer_size = 256\n",
    "        self.hidden_layer_num = 3\n",
    "\n",
    "        # Training Params\n",
    "        self.learning_rate = 3e-5\n",
    "        self.alpha = 0.005\n",
    "        self.gamma = 0.95 #discount factor\n",
    "\n",
    "        # Exploration strategy\n",
    "        self.soft_max_scaling = 0.1\n",
    "\n",
    "        # Global-Local Map\n",
    "        self.global_map_scaling = 3\n",
    "        self.local_map_size = 17\n",
    "\n",
    "        # Scalar inputs instead of map\n",
    "        self.use_scalar_input = False\n",
    "        self.relative_scalars = False\n",
    "        self.blind_agent = False\n",
    "        self.max_uavs = 1\n",
    "        #self.max_devices = 10\n",
    "\n",
    "        # Printing\n",
    "        self.print_summary = True\n",
    "        \n",
    "\n",
    "class DDQNAgent(object):\n",
    "\n",
    "    def __init__(self, params: DDQNAgentParams, example_state, example_action, stats=None): \n",
    "\n",
    "        self.params = params\n",
    "        gamma = tf.constant(self.params.gamma, dtype=float) #상수값 지정\n",
    "        self.align_counter = 0\n",
    "        \n",
    "        \n",
    "\n",
    "        self.boolean_map_shape = example_state.get_boolean_map_shape() \n",
    "        #padded_red, padded_rest를 합친 것의 shape(boolean 표현)\n",
    "        #self.float_map_shape = example_state.get_float_map_shape() \n",
    "        #padded_red, padded_rest를 합친 것의 shape(float 표현)\n",
    "        self.scalars = example_state.get_num_scalars(give_position=self.params.use_scalar_input)\n",
    "        self.num_actions = len(type(example_action)) # action 총 갯수\n",
    "\n",
    "\n",
    "\n",
    "        # Create shared inputs\n",
    "        action_input = Input(shape=(), name='action_input', dtype=tf.int64)\n",
    "        reward_input = Input(shape=(), name='reward_input', dtype=tf.float32)\n",
    "        termination_input = Input(shape=(), name='termination_input', dtype=tf.bool)\n",
    "        q_star_input = Input(shape=(), name='q_star_input', dtype=tf.float32)\n",
    "        \n",
    "\n",
    "\n",
    "        boolean_map_input = Input(shape=self.boolean_map_shape, name='boolean_map_input', dtype=tf.bool)\n",
    "        #float_map_input = Input(shape=self.float_map_shape, name='float_map_input', dtype=tf.float32)\n",
    "        scalars_input = Input(shape=(self.scalars,), name='scalars_input', dtype=tf.float32)\n",
    "        states = [boolean_map_input,\n",
    "                    #float_map_input,\n",
    "                    scalars_input]\n",
    "        self.state = states\n",
    "\n",
    "        map_cast = tf.cast(boolean_map_input, dtype=tf.float32) \n",
    "            #Boolean은 0 또는 1로 바꿔 줌, float을 정수형으로 바꿔 줌\n",
    "        #padded_map = tf.concat([map_cast, float_map_input], axis=3) \n",
    "        padded_map = map_cast\n",
    "            #z축을 기준으로 행렬matrix를 합침\n",
    "\n",
    "        self.q_network = self.build_model(padded_map, scalars_input, states)\n",
    "        self.target_network = self.build_model(padded_map, scalars_input, states, 'target_')\n",
    "            \n",
    "        self.hard_update()\n",
    "\n",
    "#        self.global_map_model = Model(inputs=[boolean_map_input, float_map_input],\n",
    "#                                    outputs=self.global_map)\n",
    "#        self.local_map_model = Model(inputs=[boolean_map_input, float_map_input],\n",
    "#                                    outputs=self.local_map)\n",
    "#        self.total_map_model = Model(inputs=[boolean_map_input, float_map_input],\n",
    "#                                    outputs=self.total_map)\n",
    "        \n",
    "        self.global_map_model = Model(inputs=[boolean_map_input],\n",
    "                                    outputs=self.global_map)\n",
    "        self.local_map_model = Model(inputs=[boolean_map_input],\n",
    "                                    outputs=self.local_map)\n",
    "        self.total_map_model = Model(inputs=[boolean_map_input],\n",
    "                                    outputs=self.total_map)        \n",
    "\n",
    "        q_values = self.q_network.output\n",
    "        q_target_values = self.target_network.output\n",
    "\n",
    "        # Define Q* in min(Q - (r + gamma_terminated * Q*))^2\n",
    "        max_action = tf.argmax(q_values, axis=1, name='max_action', output_type=tf.int64)\n",
    "        max_action_target = tf.argmax(q_target_values, axis=1, name='max_action', output_type=tf.int64)\n",
    "        one_hot_max_action = tf.one_hot(max_action, depth=self.num_actions, dtype=float)\n",
    "        q_star = tf.reduce_sum(tf.multiply(one_hot_max_action, q_target_values, name='mul_hot_target'), axis=1,\n",
    "                               name='q_star')\n",
    "        #tf.reduce_sum : axis = 1 이므로 행 단위로 sum\n",
    "        self.q_star_model = Model(inputs=states, outputs=q_star)\n",
    "\n",
    "        # Define Bellman loss\n",
    "        one_hot_rm_action = tf.one_hot(action_input, depth=self.num_actions, on_value=1.0, off_value=0.0, dtype=float)\n",
    "        one_cold_rm_action = tf.one_hot(action_input, depth=self.num_actions, on_value=0.0, off_value=1.0, dtype=float)\n",
    "        q_old = tf.stop_gradient(tf.multiply(q_values, one_cold_rm_action))\n",
    "        gamma_terminated = tf.multiply(tf.cast(tf.math.logical_not(termination_input), tf.float32), gamma)\n",
    "        q_update = tf.expand_dims(tf.add(reward_input, tf.multiply(q_star_input, gamma_terminated)), 1)\n",
    "        q_update_hot = tf.multiply(q_update, one_hot_rm_action)\n",
    "        q_new = tf.add(q_update_hot, q_old)\n",
    "        q_loss = tf.losses.MeanSquaredError()(q_new, q_values)\n",
    "        self.q_loss_model = Model(\n",
    "            inputs=states + [action_input, reward_input, termination_input, q_star_input],\n",
    "            outputs=q_loss)\n",
    "\n",
    "        # Exploit act model\n",
    "        self.exploit_model = Model(inputs=states, outputs=max_action)\n",
    "        self.exploit_model_target = Model(inputs=states, outputs=max_action_target)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.q_optimizer = tf.optimizers.Adam(learning_rate=params.learning_rate, amsgrad=True)\n",
    "\n",
    "        if self.params.print_summary:\n",
    "            self.q_loss_model.summary()\n",
    "\n",
    "        if stats:\n",
    "            stats.set_model(self.target_network)\n",
    "            \n",
    "\n",
    "    def build_model(self, map_proc, states_proc, inputs, name=''):\n",
    "\n",
    "        flatten_map = self.create_map_proc(map_proc, name)\n",
    "\n",
    "        layer = Concatenate(name=name + 'concat')([flatten_map, states_proc])\n",
    "        for k in range(self.params.hidden_layer_num):\n",
    "            layer = Dense(self.params.hidden_layer_size, activation='relu', \n",
    "                          name=name + 'hidden_layer_all_'+ str(k))(layer)\n",
    "        output = Dense(self.num_actions, activation='linear', name=name + 'output_layer')(layer)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_scalars_model(self, inputs, name=''):\n",
    "\n",
    "        layer = Concatenate(name=name + 'concat')(inputs)\n",
    "        for k in range(self.params.hidden_layer_num):\n",
    "            layer = Dense(self.params.hidden_layer_size, activation='relu', \n",
    "                          name=name + 'hidden_layer_all_' + str(k))(layer)\n",
    "        output = Dense(self.num_actions, activation='linear', name=name + 'output_layer')(layer)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_blind_model(self, inputs, name=''):\n",
    "\n",
    "        layer = inputs\n",
    "        for k in range(self.params.hidden_layer_num):\n",
    "            layer = Dense(self.params.hidden_layer_size, activation='relu', \n",
    "                          name=name + 'hidden_layer_all_' + str(k))(layer)\n",
    "        output = Dense(self.num_actions, activation='linear', name=name + 'output_layer')(layer)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def create_map_proc(self, conv_in, name):\n",
    "\n",
    "        # Forking for global and local map\n",
    "        # Global Map\n",
    "        global_map = tf.stop_gradient(#네트워크의 특정 파트만 학습\n",
    "            AvgPool2D((self.params.global_map_scaling, self.params.global_map_scaling))(conv_in)) \n",
    "        #AvgPool2D(conv_in) = stop_gradient의 input이 됨\n",
    "\n",
    "        self.global_map = global_map\n",
    "        self.total_map = conv_in\n",
    "\n",
    "        for k in range(self.params.conv_layers):\n",
    "            global_map = Conv2D(self.params.conv_kernels, self.params.conv_kernel_size, activation='relu',\n",
    "                                strides=(1, 1),\n",
    "                                name=name + 'global_conv_' + str(k + 1))(global_map)\n",
    "\n",
    "        flatten_global = Flatten(name=name + 'global_flatten')(global_map)\n",
    "\n",
    "        # Local Map\n",
    "        crop_frac = float(self.params.local_map_size) / float(self.boolean_map_shape[0])\n",
    "        local_map = tf.stop_gradient(tf.image.central_crop(conv_in, crop_frac)) \n",
    "        #이미지 중앙 영역 자르기(image, central_faction)\n",
    "        self.local_map = local_map\n",
    "\n",
    "        for k in range(self.params.conv_layers):\n",
    "            local_map = Conv2D(self.params.conv_kernels, self.params.conv_kernel_size, activation='relu',\n",
    "                               strides=(1, 1),\n",
    "                               name=name + 'local_conv_' + str(k + 1))(local_map)\n",
    "\n",
    "        flatten_local = Flatten(name=name + 'local_flatten')(local_map)\n",
    "\n",
    "        return Concatenate(name=name + 'concat_flatten')([flatten_global, flatten_local])\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.get_soft_max_exploration(state)\n",
    "\n",
    "    def get_random_action(self):\n",
    "        return np.random.randint(0, self.num_actions)\n",
    "\n",
    "    def get_exploitation_action(self, state):\n",
    "\n",
    "        #이것만 보면 된다.\n",
    "        boolean_map_in = state.get_boolean_map()[tf.newaxis, ...]\n",
    "#        float_map_in = state.get_float_map()[tf.newaxis, ...]\n",
    "        scalars = np.array(state.get_scalars(), dtype=np.single)[tf.newaxis, ...]\n",
    "\n",
    "#        return self.exploit_model([boolean_map_in, float_map_in, scalars]).numpy()[0]\n",
    "        return self.exploit_model([boolean_map_in, scalars]).numpy()[0]\n",
    "    \n",
    "\n",
    "    def get_soft_max_exploration(self, state):\n",
    "\n",
    "        q_values_exploration = self.q_network.output\n",
    "        \n",
    "        # Softmax explore model\n",
    "        softmax_scaling = tf.divide(q_values_exploration, tf.constant(self.params.soft_max_scaling, dtype=float))\n",
    "        softmax_action = tf.math.softmax(softmax_scaling, name='softmax_action')\n",
    "        soft_explore_model = Model(inputs=self.state, outputs=softmax_action)\n",
    "        boolean_map_in = state.get_boolean_map()[tf.newaxis, ...]\n",
    "     #   float_map_in = state.get_float_map()[tf.newaxis, ...]\n",
    "        scalars = np.array(state.get_scalars(), dtype=np.single)[tf.newaxis, ...]\n",
    "    #    p = soft_explore_model([boolean_map_in, float_map_in, scalars]).numpy()[0]\n",
    "        p = soft_explore_model([boolean_map_in, scalars]).numpy()[0]\n",
    "\n",
    "        return np.random.choice(range(self.num_actions), size=1, p=p)\n",
    "\n",
    "    def get_exploitation_action_target(self, state):\n",
    "\n",
    "        boolean_map_in = state.get_boolean_map()[tf.newaxis, ...]\n",
    "      #  float_map_in = state.get_float_map()[tf.newaxis, ...]\n",
    "        scalars = np.array(state.get_scalars(), dtype=np.single)[tf.newaxis, ...]\n",
    "\n",
    "       # return self.exploit_model_target([boolean_map_in, float_map_in, scalars]).numpy()[0]\n",
    "        return self.exploit_model_target([boolean_map_in, scalars]).numpy()[0]\n",
    "    \n",
    "    def hard_update(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    def soft_update(self, alpha):\n",
    "        weights = self.q_network.get_weights()\n",
    "        target_weights = self.target_network.get_weights()\n",
    "        self.target_network.set_weights(\n",
    "            [w_new * alpha + w_old * (1. - alpha) for w_new, w_old in zip(weights, target_weights)])\n",
    "\n",
    "    def train(self, experiences):\n",
    "        boolean_map = experiences[0]\n",
    "#        float_map = experiences[1]\n",
    "        scalars = tf.convert_to_tensor(experiences[1], dtype=tf.float32)\n",
    "        action = tf.convert_to_tensor(experiences[2], dtype=tf.int64)\n",
    "        reward = experiences[3]\n",
    "        next_boolean_map = experiences[4]\n",
    "#        next_float_map = experiences[6]\n",
    "        next_scalars = tf.convert_to_tensor(experiences[5], dtype=tf.float32)\n",
    "        terminated = experiences[6]\n",
    "\n",
    "        q_star = self.q_star_model([next_boolean_map, next_scalars])\n",
    "\n",
    "        # Train Value network\n",
    "        with tf.GradientTape() as tape: \n",
    "            #context안에 실행된 모든 연산을 tape에 기록\n",
    "            q_loss = self.q_loss_model([boolean_map, scalars, action, reward, terminated, q_star])\n",
    "            \n",
    "        q_grads = tape.gradient(q_loss, self.q_network.trainable_variables)\n",
    "        #위의 with tf.GradientTape로 연산 과정들을 loss에 대한 q_network.trainable_variables의 미분 실행\n",
    "        self.q_optimizer.apply_gradients(zip(q_grads, self.q_network.trainable_variables))\n",
    "        #gradient 직접 조작하기\n",
    "\n",
    "        self.soft_update(self.params.alpha)\n",
    "\n",
    "    def save_weights(self, path_to_weights):\n",
    "        self.target_network.save_weights(path_to_weights)\n",
    "\n",
    "    def save_model(self, path_to_model):\n",
    "        self.target_network.save(path_to_model)\n",
    "\n",
    "    def load_weights(self, path_to_weights):\n",
    "        #print('1stage: ', self.q_network.get_weights())\n",
    "        self.q_network.load_weights(path_to_weights)\n",
    "        #print('2stage: ', self.q_network.get_weights())\n",
    "        self.hard_update()\n",
    "\n",
    "    def get_global_map(self, state):\n",
    "        boolean_map_in = state.get_boolean_map()[tf.newaxis, ...] #차원 변경(추가하고 싶은 위치에 tf.newaxis)\n",
    "   #     float_map_in = state.get_float_map()[tf.newaxis, ...]\n",
    "  #     return self.global_map_model([boolean_map_in, float_map_in]).numpy() \n",
    "        return self.global_map_model([boolean_map_in]).numpy()\n",
    "    #numpy() : 텐서를 넘파이 배열로 변환\n",
    "\n",
    "    def get_local_map(self, state):\n",
    "        boolean_map_in = state.get_boolean_map()[tf.newaxis, ...]\n",
    "    #    float_map_in = state.get_float_map()[tf.newaxis, ...]\n",
    "   #     return self.local_map_model([boolean_map_in, float_map_in]).numpy()\n",
    "        return self.local_map_model([boolean_map_in]).numpy()\n",
    "\n",
    "    def get_total_map(self, state):\n",
    "        boolean_map_in = state.get_boolean_map()[tf.newaxis, ...]\n",
    "    #    float_map_in = state.get_float_map()[tf.newaxis, ...]\n",
    "    #    return self.total_map_model([boolean_map_in, float_map_in]).numpy()\n",
    "        return self.total_map_model([boolean_map_in]).numpy()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
